# ğŸš€ NyayaShastra - Local Hybrid RAG Pipeline

## Architecture Overview

This upgrade transforms NyayaShastra from a cloud-dependent RAG system to a **fully local, high-performance Hybrid RAG pipeline** optimized for legal document retrieval.

### Why This Upgrade?

**Previous Issues:**
- âŒ BM25 (keyword search) misses semantic meaning
- âŒ Cloud APIs introduce latency and rate limits
- âŒ Character-based chunking breaks legal sections
- âŒ Hallucinations from poor context filtering

**New Solution:**
- âœ… Hybrid Search: Dense (semantic) + Sparse (keyword) retrieval
- âœ… Local models: No cloud dependencies, no rate limits
- âœ… Semantic chunking: Preserves legal section boundaries
- âœ… Re-ranking: Filters out hallucination sources before LLM sees them

---

## ğŸ“Š Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PHASE 1: DATA INGESTION                      â”‚
â”‚                  (Better Input = Better Output)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  PDF â†’ Markdown Conversion                         â”‚
    â”‚  (Preserves headers like ## Section 302)           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Semantic Chunking                                 â”‚
    â”‚  - Meaning-based splits, not just 1000 chars       â”‚
    â”‚  - Preserves legal section boundaries              â”‚
    â”‚  - Extracts metadata (Act, Section Number)         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  BGE-M3 Embeddings                                 â”‚
    â”‚  - Multi-lingual (Hindi + English)                 â”‚
    â”‚  - Long context (8192 tokens)                      â”‚
    â”‚  - 1024-dim dense vectors                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ChromaDB Storage                                  â”‚
    â”‚  Collection: legal_documents_semantic              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PHASE 2: CONTEXT AWARENESS                      â”‚
â”‚                   (The "BERT" Upgrade)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    User Query: "What is the punishment for murder?"
                              â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                                         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Dense Retrieval â”‚                  â”‚Sparse Retrieval â”‚
    â”‚   (BGE-M3)      â”‚                  â”‚     (BM25)      â”‚
    â”‚ "Semantic       â”‚                  â”‚ "Section 302"   â”‚
    â”‚  meaning"       â”‚                  â”‚ exact matches   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                         â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Hybrid Fusion         â”‚
              â”‚  Top 20 candidates     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  BGE-Reranker-v2-m3    â”‚
              â”‚  "Strict Judge"        â”‚
              â”‚  Re-scores all pairs   â”‚
              â”‚  Filters low quality   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Top 5 High-Quality    â”‚
              â”‚  Contexts              â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PHASE 3: THE BRAIN                            â”‚
â”‚                   (Local Generation)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Top 5 Contexts        â”‚
              â”‚  + System Prompt       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Ollama                â”‚
              â”‚  Llama-3-8B-Instruct   â”‚
              â”‚  (4-bit quantized)     â”‚
              â”‚  Runs on i7-13620H     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Reliable Answer       â”‚
              â”‚  No Hallucinations     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Components

### Phase 1: Data Ingestion

#### 1. **Semantic Chunking Service** (`chunking_service.py`)
- **Purpose**: Intelligent document splitting that preserves meaning
- **Features**:
  - Section-based chunking (respects legal structure)
  - Metadata extraction (Act Name, Section Numbers)
  - Markdown structure preservation
  - Configurable chunk size and overlap

#### 2. **BGE-M3 Embedding Service** (`embedding_service.py`)
- **Model**: `BAAI/bge-m3`
- **Features**:
  - Multi-lingual: Hindi + English support
  - Long context: 8192 tokens
  - Output: 1024-dimensional dense vectors
  - Fast inference with FP16
- **Why BGE-M3?**: State-of-the-art (2026) for multi-lingual retrieval

#### 3. **Ingestion Script** (`scripts/ingest_semantic.py`)
- Converts PDFs to Markdown
- Applies semantic chunking
- Generates BGE-M3 embeddings
- Stores in ChromaDB with metadata

### Phase 2: Context Awareness

#### 4. **Hybrid Search Service** (`hybrid_search_service.py`)
- **Dense Retrieval**: BGE-M3 semantic search
- **Sparse Retrieval**: BM25 keyword search
- **Fusion**: Weighted combination of both
- **Why Hybrid?**:
  - Query: "Section 302" â†’ BM25 finds exact matches
  - Query: "Killing someone punishment" â†’ BGE-M3 finds semantic matches

#### 5. **Re-ranker Service** (`reranker_service.py`)
- **Model**: `BAAI/bge-reranker-v2-m3`
- **Purpose**: Cross-encoder that acts as a "strict judge"
- **How it works**:
  1. Reads user query + each candidate document
  2. Scores relevance (0-1)
  3. Filters low-quality matches (< threshold)
  4. Returns only top K high-quality contexts
- **Critical**: Prevents hallucinations by eliminating bad contexts before LLM sees them

### Phase 3: The Brain

#### 6. **Ollama Service** (`ollama_service.py`)
- **Model**: Llama-3-8B-Instruct (4-bit quantized)
- **Why Local?**:
  - No API costs
  - No rate limits
  - Full privacy
  - Runs on your i7-13620H (10 cores)
- **Quantization**: 4-bit reduces 16GB model to ~6GB (fits in RAM)

#### 7. **Enhanced Retriever Service** (`enhanced_retriever_service.py`)
- Orchestrates the entire pipeline
- Query classification
- Domain detection
- Hybrid search + re-ranking integration

---

## ğŸ“¦ Installation

### 1. Install Python Dependencies

```bash
cd backend
pip install -r requirements.txt
```

**Key new dependencies:**
- `FlagEmbedding>=1.2.0` - BGE-M3 + Reranker
- `rank-bm25>=0.2.2` - BM25 sparse retrieval
- `langchain-experimental` - Semantic chunking
- `httpx` - Ollama API client

### 2. Install Ollama

#### Linux:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

#### macOS:
```bash
brew install ollama
```

#### Windows:
Download from https://ollama.ai/download

### 3. Download Llama-3-8B (Quantized)

```bash
# Start Ollama server (in a separate terminal)
ollama serve

# Pull the 4-bit quantized model (recommended)
ollama pull llama3:8b-instruct-q4_K_M

# Alternative: Full precision (slower)
ollama pull llama3:8b-instruct
```

**Model sizes:**
- `q4_K_M` (4-bit): ~4.7GB download, ~6GB RAM usage
- Full precision: ~14GB download, ~16GB RAM usage

---

## ğŸš€ Usage

### Step 1: Ingest Documents with Semantic Chunking

```bash
cd backend
python scripts/ingest_semantic.py
```

**What it does:**
- Scans `backend/data/` folders
- Converts PDFs to Markdown
- Creates semantic chunks
- Generates BGE-M3 embeddings
- Stores in ChromaDB collection: `legal_documents_semantic`

**Expected output:**
```
ğŸš€ SEMANTIC PDF INGESTION
ğŸ“‚ Scanning directory: /path/to/data
ğŸ“„ Found 150 PDF files

[1/150] Processing: Criminal/IPC.pdf
   Domain: criminal
   â¤ Converting to Markdown...
   â¤ Extracted 45000 characters
   â¤ Semantic chunking...
   â¤ Created 35 semantic chunks
   â¤ Generating BGE-M3 embeddings...
   âœ… Indexed 35 chunks

...

âœ… INGESTION COMPLETE
Total chunks indexed: 2500
Domain breakdown:
  - criminal       : 800 chunks
  - traffic        : 500 chunks
  - civil_family   : 400 chunks
  ...
```

### Step 2: Test Hybrid Search

```python
from app.services.hybrid_search_service import get_hybrid_search_service

# Initialize service
search_service = get_hybrid_search_service()

# Perform hybrid search
results = search_service.hybrid_search(
    query="What is the punishment for murder?",
    n_results=5,
    use_reranking=True,
    domain_filter="criminal"  # Optional
)

# Display results
for i, doc in enumerate(results, 1):
    print(f"\n{i}. Score: {doc['rerank_score']:.4f}")
    print(f"   Domain: {doc['metadata']['domain']}")
    print(f"   Source: {doc['metadata']['source']}")
    print(f"   Content: {doc['content'][:200]}...")
```

### Step 3: Test Ollama Integration

```python
import asyncio
from app.services.ollama_service import get_ollama_service

async def test():
    # Initialize Ollama
    ollama = await get_ollama_service()
    
    # Generate with context (RAG)
    contexts = [
        {
            "content": "Section 302 IPC: Whoever commits murder shall be punished with death or life imprisonment.",
            "metadata": {"source": "IPC.pdf", "sections": "302"}
        }
    ]
    
    response = await ollama.generate_with_context(
        query="What is the punishment for murder?",
        contexts=contexts
    )
    
    print(response)

asyncio.run(test())
```

### Step 4: Full Pipeline Test

```python
import asyncio
from app.services.enhanced_retriever_service import get_enhanced_retriever_service
from app.services.ollama_service import get_ollama_service

async def full_pipeline_test():
    # Initialize services
    retriever = await get_enhanced_retriever_service()
    ollama = await get_ollama_service()
    
    # User query
    query = "What is the punishment for murder under IPC?"
    
    # Step 1: Hybrid retrieval
    result = await retriever.retrieve(query, n_results=5)
    
    print(f"Retrieved {len(result.documents)} documents")
    
    # Step 2: Generate answer with Ollama
    answer = await ollama.generate_with_context(
        query=query,
        contexts=result.documents
    )
    
    print(f"\nQuery: {query}")
    print(f"\nAnswer: {answer}")

asyncio.run(full_pipeline_test())
```

---

## ğŸ¯ Performance Expectations

### Hardware: i7-13620H (10 cores, 16 threads)

| Component | Speed | Resource Usage |
|-----------|-------|----------------|
| **BGE-M3 Embedding** | ~50-100 docs/sec | ~2GB RAM |
| **BM25 Search** | <50ms | <1GB RAM |
| **BGE-Reranker** | ~10-20 pairs/sec | ~1.5GB RAM |
| **Llama-3-8B (4-bit)** | ~20-30 tokens/sec | ~6GB RAM |
| **Total Pipeline** | ~2-3 sec/query | ~10GB RAM |

**Expected Performance:**
- Cold start (first query): 5-10 seconds (model loading)
- Warm queries: 2-3 seconds
- Streaming responses: Real-time token delivery

---

## ğŸ” Comparison: Old vs New

| Aspect | Old System | New System |
|--------|-----------|------------|
| **Chunking** | Fixed 800 chars | Semantic (meaning-based) |
| **Embeddings** | Sentence-Transformers | BGE-M3 (Multi-lingual, 8k ctx) |
| **Search** | Vector only | Hybrid (Vector + BM25) |
| **Re-ranking** | âŒ None | âœ… BGE-Reranker (filters bad contexts) |
| **LLM** | Cloud APIs (OpenAI/Groq) | Local Ollama (Llama-3-8B) |
| **Latency** | Variable (network dependent) | Consistent (local) |
| **Cost** | Per-token pricing | One-time setup |
| **Privacy** | Data sent to cloud | 100% local |
| **Rate Limits** | âœ… Yes | âŒ No limits |

---

## ğŸ› Troubleshooting

### Ollama Not Running
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# If not, start it
ollama serve
```

### Model Not Found
```bash
# List installed models
ollama list

# Pull the model if missing
ollama pull llama3:8b-instruct-q4_K_M
```

### Out of Memory
- Use 4-bit quantized model: `llama3:8b-instruct-q4_K_M`
- Close other applications
- Reduce batch size in embedding service

### Slow Inference
- Make sure you're using FP16: `use_fp16=True`
- Use 4-bit quantization for Llama-3
- Check CPU usage (should use all cores)

---

## ğŸ“š Fine-Tuning Strategy (Phase 4 - Future)

**CRITICAL: Do NOT fine-tune the model on law text!**

âŒ **Wrong Approach:**
- Fine-tuning on IPC/BNS text â†’ Model memorizes imperfectly â†’ Hallucinations

âœ… **Correct Approach:**
- Fine-tune on **Legal Reasoning Style**
- Teach HOW to answer (tone, citation format, Hinglish)
- Let RAG provide the actual facts

**Tool**: Unsloth (https://github.com/unslothai/unsloth)
- 2x faster fine-tuning
- 70% less memory
- Works with Llama-3

**Dataset Format:**
```json
[
  {
    "instruction": "User query about legal matter",
    "input": "Context from retrieved documents",
    "output": "Properly formatted answer with citations"
  }
]
```

---

## ğŸ‰ Summary

You now have a **production-ready, fully local Hybrid RAG pipeline** that:

1. âœ… **Phase 1**: Ingests PDFs with semantic chunking and BGE-M3 embeddings
2. âœ… **Phase 2**: Retrieves with hybrid search (Dense + Sparse) and re-ranks to eliminate hallucinations
3. âœ… **Phase 3**: Generates answers locally with Llama-3-8B via Ollama

**No cloud dependencies. No rate limits. No hallucinations. No costs.**

---

## ğŸ“ Next Steps

1. Run ingestion: `python scripts/ingest_semantic.py`
2. Start Ollama: `ollama serve`
3. Test pipeline: Run the test scripts above
4. Integrate with your FastAPI backend
5. (Optional) Fine-tune on reasoning style

**Questions?** Check the troubleshooting section or review individual service documentation.
